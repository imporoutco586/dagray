[32m2024-06-11 15:02:41 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 885abc65-1292-4044-ac84-808309188086 - 834313 - LOGS_CAPTURED - Started capturing logs in process (pid: 834313).
[32m2024-06-11 15:02:41 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 885abc65-1292-4044-ac84-808309188086 - 834313 - trainer - STEP_START - Started execution of step "trainer".
2024-06-11 15:02:42,325	INFO worker.py:1567 -- Connecting to existing Ray cluster at address: 10.16.22.178:6379...
2024-06-11 15:02:43,078	INFO worker.py:1743 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
2024-06-11 15:02:44,289	INFO tune.py:263 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `<FrameworkTrainer>(...)`.
2024-06-11 15:02:44,291	INFO tune.py:622 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
[33m(raylet)[0m Traceback (most recent call last):
[33m(raylet)[0m   File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/workers/default_worker.py", line 7, in <module>
[33m(raylet)[0m     import ray
[33m(raylet)[0m     from ray._private.worker import (  # noqa: E402,F401
[33m(raylet)[0m     from ray._private.runtime_env.py_modules import upload_py_modules_if_needed
[33m(raylet)[0m     from ray._private.runtime_env.working_dir import set_pythonpath_in_context
[33m(raylet)[0m     scratch_dir: Optional[str] = os.getcwd(),
[33m(raylet)[0m FileNotFoundError: [Errno 2] No such file or directory
[33m(raylet)[0m [2024-06-11 15:03:45,540 E 2228941 2228941] (raylet) worker_pool.cc:550: Some workers of the worker process(834526) have not registered within the timeout. The process is dead, probably it crashed during start.
[33m(raylet)[0m Traceback (most recent call last):
[33m(raylet)[0m   File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/runtime_env/working_dir.py", line 31, in <module>[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[33m(raylet)[0m     import ray
[33m(raylet)[0m     from ray._private.worker import (  # noqa: E402,F401
[33m(raylet)[0m     from ray._private.runtime_env.py_modules import upload_py_modules_if_needed
[33m(raylet)[0m     from ray._private.runtime_env.working_dir import set_pythonpath_in_context
[33m(raylet)[0m     scratch_dir: Optional[str] = os.getcwd(),
[33m(raylet)[0m FileNotFoundError: [Errno 2] No such file or directory
[33m(raylet)[0m Traceback (most recent call last):
[33m(raylet)[0m     import ray
[33m(raylet)[0m     from ray._private.worker import (  # noqa: E402,F401
[33m(raylet)[0m     from ray._private.runtime_env.py_modules import upload_py_modules_if_needed
[33m(raylet)[0m     from ray._private.runtime_env.working_dir import set_pythonpath_in_context
[33m(raylet)[0m     scratch_dir: Optional[str] = os.getcwd(),
[33m(raylet)[0m FileNotFoundError: [Errno 2] No such file or directory
[33m(raylet)[0m [2024-06-11 15:04:45,560 E 2228941 2228941] (raylet) worker_pool.cc:550: Some workers of the worker process(835560) have not registered within the timeout. The process is dead, probably it crashed during start.[32m [repeated 2x across cluster][0m
[33m(raylet)[0m   File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/runtime_env/working_dir.py", line 31, in <module>[32m [repeated 5x across cluster][0m
[33m(raylet)[0m Traceback (most recent call last):
[33m(raylet)[0m     import ray
[33m(raylet)[0m     from ray._private.worker import (  # noqa: E402,F401
[33m(raylet)[0m     from ray._private.runtime_env.py_modules import upload_py_modules_if_needed
[33m(raylet)[0m     from ray._private.runtime_env.working_dir import set_pythonpath_in_context
[33m(raylet)[0m     scratch_dir: Optional[str] = os.getcwd(),
[33m(raylet)[0m FileNotFoundError: [Errno 2] No such file or directory
[33m(raylet)[0m [2024-06-11 15:05:46,231 E 2228941 2228941] (raylet) worker_pool.cc:550: Some workers of the worker process(836651) have not registered within the timeout. The process is dead, probably it crashed during start.
[33m(raylet)[0m   File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/runtime_env/working_dir.py", line 31, in <module>[32m [repeated 5x across cluster][0m
[33m(raylet)[0m Traceback (most recent call last):
[33m(raylet)[0m     import ray
[33m(raylet)[0m     from ray._private.worker import (  # noqa: E402,F401
[33m(raylet)[0m     from ray._private.runtime_env.py_modules import upload_py_modules_if_needed
[33m(raylet)[0m     from ray._private.runtime_env.working_dir import set_pythonpath_in_context
[33m(raylet)[0m     scratch_dir: Optional[str] = os.getcwd(),
[33m(raylet)[0m FileNotFoundError: [Errno 2] No such file or directory
2024-06-11 15:05:56,674	WARNING tune.py:229 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. 
2024-06-11 15:05:56,678	INFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/kaiyuan/ray_results/TorchTrainer_2024-06-11_15-02-42' in 0.0028s.
[32m2024-06-11 15:05:57 +0800[0m - dagster - [34mERROR[0m - [31mtrainer_job - 885abc65-1292-4044-ac84-808309188086 - 834313 - trainer - STEP_FAILURE - Execution of step "trainer" failed.

dagster._core.errors.DagsterExecutionInterruptedError

Stack Trace:
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/execute_plan.py", line 286, in dagster_event_sequence_for_step
    for step_event in check.generator(step_events):
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/execute_step.py", line 525, in core_dagster_event_sequence_for_step
    for user_event in _step_output_error_checked_user_event_sequence(
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/execute_step.py", line 202, in _step_output_error_checked_user_event_sequence
    for user_event in user_event_sequence:
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/execute_step.py", line 100, in _process_asset_results_to_events
    for user_event in user_event_sequence:
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/compute.py", line 214, in execute_core_compute
    for step_output in _yield_compute_results(step_context, inputs, compute_fn, compute_context):
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/compute.py", line 183, in _yield_compute_results
    for event in iterate_with_context(
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_utils/__init__.py", line 463, in iterate_with_context
    next_output = next(iterator)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/compute_generator.py", line 131, in _coerce_op_compute_fn_to_iterator
    result = invoke_compute_fn(
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_core/execution/plan/compute_generator.py", line 125, in invoke_compute_fn
    return fn(context, **args_to_pass) if context_arg_provided else fn(**args_to_pass)
  File "/home/kaiyuan/Desktop/dag_ray/dagster-quickstart/dagster_quickstart/assets.py", line 166, in trainer
    results = trainer.fit()
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/train/base_trainer.py", line 623, in fit
    result_grid = tuner.fit()
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/tuner.py", line 379, in fit
    return self._local_tuner.fit()
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/impl/tuner_internal.py", line 477, in fit
    analysis = self._fit_internal(trainable, param_space)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/impl/tuner_internal.py", line 596, in _fit_internal
    analysis = run(
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/tune.py", line 1033, in run
    runner.cleanup()
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/execution/tune_controller.py", line 1976, in cleanup
    self._cleanup_trials()
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/execution/tune_controller.py", line 793, in _cleanup_trials
    self._schedule_trial_stop(trial)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/execution/tune_controller.py", line 1404, in _schedule_trial_stop
    self._remove_actor(tracked_actor=tracked_actor)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/tune/execution/tune_controller.py", line 817, in _remove_actor
    if self._actor_manager.remove_actor(
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/air/execution/_internal/actor_manager.py", line 638, in remove_actor
    self._resource_manager.cancel_resource_request(
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/air/execution/resources/placement_group.py", line 169, in cancel_resource_request
    ray.util.remove_placement_group(pg)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 141, in wrapper
    return func(*args, **kwargs)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/ray/util/placement_group.py", line 235, in remove_placement_group
    worker.core_worker.remove_placement_group(placement_group.id)
  File "/home/kaiyuan/anaconda3/envs/py38/lib/python3.8/site-packages/dagster/_utils/interrupts.py", line 82, in _new_signal_handler
    raise error_cls()
[0m
