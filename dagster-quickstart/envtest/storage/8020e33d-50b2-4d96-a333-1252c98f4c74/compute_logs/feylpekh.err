[32m2024-07-17 10:57:45 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - 2947722 - LOGS_CAPTURED - Started capturing logs in process (pid: 2947722).
[32m2024-07-17 10:57:45 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - 2947722 - trainer - STEP_START - Started execution of step "trainer".
2024-07-17 10:57:46,050	INFO worker.py:1567 -- Connecting to existing Ray cluster at address: 10.16.22.178:6379...
2024-07-17 10:57:46,059	INFO worker.py:1743 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
2024-07-17 10:57:46,136	INFO tune.py:263 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `<FrameworkTrainer>(...)`.
2024-07-17 10:57:46,138	INFO tune.py:622 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
[36m(TorchTrainer pid=2947836)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=2947836)[0m - (ip=10.16.22.178, pid=2947926) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=2947836)[0m - (ip=10.16.22.178, pid=2947927) world_rank=1, local_rank=1, node_rank=0
[36m(RayTrainWorker pid=2947926)[0m Setting up process group for: env:// [rank=0, world_size=2]
[36m(RayTrainWorker pid=2947926)[0m Moving model to device: cpu
[36m(RayTrainWorker pid=2947926)[0m Wrapping provided model in DistributedDataParallel.
2024-07-17 10:58:28,628	WARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).
2024-07-17 10:58:28,629	INFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/kaiyuan/ray_results/TorchTrainer_2024-07-17_10-57-46' in 0.0033s.
[32m2024-07-17 10:58:28 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - 2947722 - trainer - STEP_OUTPUT - Yielded output "result" of type "Any". (Type check passed).
[32m2024-07-17 10:58:28 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - trainer - Writing file at: /home/kaiyuan/Desktop/dag_ray/dagster-quickstart/envtest/storage/trainer using PickledObjectFilesystemIOManager...
[32m2024-07-17 10:58:28 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - 2947722 - trainer - ASSET_MATERIALIZATION - Materialized value trainer.
[32m2024-07-17 10:58:28 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - 2947722 - trainer - HANDLED_OUTPUT - Handled output "result" using IO manager "io_manager"
[32m2024-07-17 10:58:28 +0800[0m - dagster - [34mDEBUG[0m - trainer_job - 8020e33d-50b2-4d96-a333-1252c98f4c74 - 2947722 - trainer - STEP_SUCCESS - Finished execution of step "trainer" in 42.76s.
